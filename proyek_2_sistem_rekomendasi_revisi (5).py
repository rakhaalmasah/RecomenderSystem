# -*- coding: utf-8 -*-
"""Proyek 2 Sistem Rekomendasi_Revisi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kRI-6mdmW2vGqRl3W4N2nJh2L2pp7lYG

# Proyek 2 Sistem Rekomendasi :
- **Nama:** Muhammad Rakha Almasah
- **Email:** muh.rakha.al@gmail.com
- **ID Dicoding:** muhrakhaal

#**Data Understanding**

Pada tahap ini, saya memahami struktur dan karakteristik data dari tiga file yang digunakan:

1. Books.csv
2. Ratings.cs
3. Users.csv

Berikut adalah penjelasan kode dan insight yang diperoleh:
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""**Memuat Dataset**

Dataset yang dimuat:

Books.csv: Informasi tentang buku (judul, penulis, tahun publikasi, penerbit, dan URL gambar).

Ratings.csv: Data rating buku yang diberikan oleh pengguna.

Users.csv: Informasi tentang pengguna, termasuk lokasi dan usia.
"""

books = pd.read_csv('/content/Books.csv')
ratings = pd.read_csv('/content/Ratings.csv')
users = pd.read_csv('/content/Users.csv')

"""## **Memahami Dataset Books**

Hasil:
1. Dataset Books memiliki 271,360 entri dan 8 kolom.
2. Tidak ada duplikasi data.
3. Ada beberapa nilai NaN:
  - Book-Author: 2 nilai kosong.
  - Publisher: 2 nilai kosong.
  - Image-URL-L: 3 nilai kosong.

Insight:
Informasi buku sebagian besar lengkap, tetapi terdapat beberapa data kosong pada kolom Book-Author dan Publisher, yang perlu ditangani pada tahap preprocessing.
"""

print(books.shape)
print('Jumlah Data Buku : ', len(ratings['ISBN'].unique()))
print(books.columns)

books.info()

books.head()

books.describe()

print("Jumlah duplikasi: ", books.duplicated().sum())
print(books.isnull().sum())

"""## **Memahami Dataset Ratings**

Hasil:
1. Dataset Ratings memiliki 1,149,780 entri dan 3 kolom:
  - User-ID: ID pengguna.
  - ISBN: ID buku.
  - Book-Rating: Nilai rating yang diberikan pengguna.
2. Tidak ada duplikasi atau nilai kosong.

Insight: Dataset ini cukup lengkap tanpa masalah data yang hilang.
Terdapat 105,283 pengguna unik yang memberikan rating untuk buku.
"""

print(ratings.shape)
print('Jumlah Data Rating : ', len(ratings['User-ID'].unique()))
print(ratings.columns)

ratings.info()

ratings.head()

ratings.describe()

print("Jumlah duplikasi: ", ratings.duplicated().sum())
print(ratings.isnull().sum())

"""## **Memahami Dataset Users**

Hasil:
1. Dataset Users memiliki 278,858 entri dan 3 kolom:
  - User-ID: ID pengguna.
  - Location: Lokasi pengguna.
  - Age: Usia pengguna (kolom ini mengandung nilai kosong).
2. Tidak ada duplikasi data.
3. Kolom Age memiliki 110,762 nilai kosong.

Insight: Sebagian besar data pengguna lengkap, tetapi kolom Age memiliki banyak nilai kosong yang perlu ditangani.
"""

print(users.shape)
print('Jumlah Data Users : ', len(users['User-ID'].unique()))
print(users.columns)

users.info()

users.head()

users.describe()

print("Jumlah duplikasi: ", users.duplicated().sum())
print(users.isnull().sum())

"""## **Insight Keseluruhan:**

1. Dataset Books, Ratings, dan Users cukup besar, dengan lebih dari 1 juta rating dari 105,283 pengguna unik untuk 271,360 buku.
2. Ada beberapa masalah data yang perlu diperbaiki:
  - Nilai kosong pada kolom Book-Author, Publisher, dan Age.
3. Dataset ini cukup kaya untuk membangun sistem rekomendasi, baik berbasis konten maupun collaborative filtering.

# **Data Preprocessing dan Data Preparation**
Pada tahap ini, data dari file Books.csv, Ratings.csv, dan Users.csv diproses lebih lanjut untuk memastikan kelengkapan, konsistensi, dan kompatibilitas antar dataset.

Berikut penjelasan setiap langkah dan hasilnya:

## **Memeriksa dan Membersihkan Serta Menghapus Kolom yang Tidak Diperlukan di Dataset Books**
Hasil:
1. Dataset Books memiliki nilai kosong di kolom:
  - Book-Author (2 nilai).
  - Publisher (2 nilai).
  - Image-URL-L (3 nilai).
2. Nilai kosong dihapus menggunakan dropna(), sehingga dataset menjadi bersih dari nilai NaN.
3. Kolom Image-URL-S, Image-URL-M, dan Image-URL-L dihapus karena tidak relevan untuk analisis dan rekomendasi.

Insight:
1. Menghapus baris dengan nilai kosong adalah pendekatan yang sederhana untuk memastikan integritas data.
2. Penghapusan kolom ini membantu mengurangi ukuran dataset dan mempermudah proses analisis selanjutnya.
"""

print("Jumlah nilai NaN di setiap kolom sebelum dibersihkan:")
print(books.isnull().sum())

print("\nBaris dengan nilai NaN:")
print(books[books.isnull().any(axis=1)])

books_cleaned = books.dropna()

print("\nJumlah nilai NaN di setiap kolom setelah dibersihkan:")
print(books_cleaned.isnull().sum())

books_cleaned = books_cleaned.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'])

books_cleaned.info()

"""## **Memeriksa dan Membersihkan Dataset Users**
Hasil:
1. Kolom Age memiliki 110,762 nilai kosong dan dihapus karena banyaknya data yang hilang.
2. Dataset Users menjadi bersih tanpa nilai NaN.

Insight: Menghapus kolom Age adalah pilihan yang tepat karena jumlah nilai kosong terlalu banyak untuk diprediksi atau diisi dengan nilai default.

"""

print("Jumlah nilai NaN di setiap kolom sebelum dibersihkan:")
print(users.isnull().sum())

users_cleaned = users.drop(columns=['Age'])

print("\nJumlah nilai NaN di setiap kolom setelah penghapusan kolom Age:")
print(users_cleaned.isnull().sum())

"""## **Menggabungkan dan Menyimpan Dataset**

Hasil:
1. Dataset Ratings digabungkan dengan Books berdasarkan kolom ISBN.
2. Dataset hasil penggabungan ini kemudian digabungkan dengan Users berdasarkan kolom User-ID.
3. Dataset gabungan memiliki 1,031,128 entri dan 8 kolom:
  - User-ID: ID pengguna.
  - ISBN: ID buku.
  - Book-Rating: Rating yang diberikan pengguna.
  - Book-Title, Book-Author, Year-Of-Publication, Publisher: Informasi buku.
  - Location: Lokasi pengguna.
4. Dataset gabungan disimpan dengan nama Ratings_Books_Users_Cleaned.csv untuk digunakan pada tahap berikutnya.

Insight:
1. Dataset gabungan ini lengkap dan siap digunakan untuk proses modeling sistem rekomendasi.
2. Dataset yang sudah dibersihkan dan digabungkan sangat kaya informasi, menghubungkan pengguna, buku, dan rating dalam satu kerangka kerja.
"""

ratings_books = ratings.merge(books_cleaned, on='ISBN', how='inner')

ratings_books_users = ratings_books.merge(users_cleaned, on='User-ID', how='inner')

ratings_books_users.info()

ratings_books_users.head()

ratings_books_users.to_csv("Ratings_Books_Users_Cleaned.csv", index=False)

"""## **Insight Keseluruhan Tahap 2:**
1. Dataset Books dan Users berhasil dibersihkan dari nilai kosong.
2. Dataset Books telah disederhanakan dengan menghapus kolom yang tidak relevan.
3. Dataset Ratings, Books, dan Users berhasil digabungkan, menghasilkan dataset yang kaya informasi dan siap untuk modeling.
4. Dataset gabungan disimpan untuk efisiensi, sehingga tidak perlu melakukan preprocessing ulang di setiap eksekusi.

# **Model Collaborative Filtering**

**Deskripsi Tahap:**

Pada tahap ini, saya membangun model rekomendasi berbasis Collaborative Filtering menggunakan PyTorch. Model ini mengandalkan embedding untuk pengguna dan buku guna memprediksi tingkat relevansi antara pengguna dan buku berdasarkan data historis rating.

**Penjelasan Kode:**
1. **Data Preparation:**
  - Dataset Ratings_Books_Users_Cleaned.csv dimuat dan diolah.
  - Dibuat indeks numerik untuk pengguna (user_idx) dan buku (book_idx) agar dapat diproses oleh model.
  - Rating buku dinormalisasi ke skala 0-1 untuk mempercepat konvergensi model.
2. **Visualisasi Aktivitas Pengguna:**
  - Menampilkan pengguna paling aktif dan paling jarang memberikan rating.
  - Plot horizontal untuk memvisualisasikan jumlah rating dari pengguna.
3. **Arsitektur Model:**
  - Model memiliki embedding untuk pengguna dan buku, dengan dimensi embedding yang ditentukan (embedding_dim = 100).
  - Embedding pengguna dan buku digabungkan, lalu diproses melalui lapisan fully connected (dense layer) dengan aktivasi ReLU.
  - Output akhir adalah nilai prediksi relevansi.
3. **Weighted Loss Function:**
  - Weighted Mean Squared Error digunakan sebagai fungsi loss untuk memberikan bobot lebih pada item dengan rating lebih tinggi.
4. **Training dan Validasi:**
  - Dataset dibagi menjadi data latih (80%) dan data validasi (20%).
  - Early stopping diterapkan untuk mencegah overfitting jika validasi loss tidak membaik selama beberapa epoch.
5. **Evaluasi Model:**
  - Plot perbandingan antara training loss dan validation loss untuk memvisualisasikan performa model selama training.
7. **Fungsi Rekomendasi:**
  - Mengambil data pengguna tertentu dan menampilkan 5 buku dengan rating tertinggi yang diberikan oleh pengguna tersebut.
  - Memberikan rekomendasi 10 buku yang relevan berdasarkan prediksi model.

**Insight dari Model Collaborative Filtering:**

1. **Visualisasi Pengguna:**
  - Grafik pengguna menunjukkan bahwa beberapa pengguna sangat aktif memberikan rating, sedangkan sebagian besar lainnya memberikan sedikit rating. Hal ini merupakan pola yang umum dalam dataset rekomendasi.
2. **Evaluasi Model:**
- Dengan early stopping, saya melihat bahwa training loss terus menurun, tetapi validation loss mulai meningkat setelah beberapa epoch. Ini menunjukkan bahwa model mulai overfit jika dilatih terlalu lama.
3. **Rekomendasi:**
- Fungsi rekomendasi menunjukkan kemampuan model untuk menyarankan buku yang relevan berdasarkan embedding pengguna dan buku. Buku-buku yang direkomendasikan didasarkan pada kesamaan pola rating di antara pengguna.

**Contoh Hasil Rekomendasi:**

**1. Pengguna dengan ID: 11676**
  - **Top 5 Buku yang Diberi Rating Tertinggi:**
    - MARBLE HEART oleh Gretta Mulrooney [Rating: 10.0].
    - His After Hours Mistress oleh Amanda Browning [Rating: 10.0].
    - ...
  - **Rekomendasi:**
    - Harry Potter and the Order of the Phoenix oleh J.K. Rowling.
    - Dude, Where’s My Country oleh Michael Moore.
    - ...
**2. Pengguna dengan ID: 153662**
  - **Top 5 Buku yang Diberi Rating Tertinggi:**
    - Second Sight oleh Unknown Author [Rating: 10.0].
    - Princess Diana: Her Life Story oleh Richard Buskin [Rating: 10.0].
    - ...
  - **Rekomendasi:**
    - Harry Potter and the Chamber of Secrets oleh J.K. Rowling.
    - Ender’s Game oleh Orson Scott Card.
    - ...

Dengan pendekatan ini, model berhasil memberikan rekomendasi yang relevan untuk berbagai pengguna berdasarkan historis rating mereka.
"""

from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.optim as optim

data = pd.read_csv('Ratings_Books_Users_Cleaned.csv')

user_activity = data.groupby('User-ID').size().reset_index(name='Rating Count')
top_active_users = user_activity.sort_values(by='Rating Count', ascending=False).head(10)
least_active_users = user_activity.sort_values(by='Rating Count', ascending=True).head(10)

plt.figure(figsize=(12, 6))
plt.barh(top_active_users['User-ID'].astype(str), top_active_users['Rating Count'], color='skyblue')
plt.xlabel('Jumlah Rating')
plt.ylabel('User-ID')
plt.title('Top 10 Pengguna Paling Aktif Memberikan Rating')
plt.gca().invert_yaxis()
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()

plt.figure(figsize=(12, 6))
plt.barh(least_active_users['User-ID'].astype(str), least_active_users['Rating Count'], color='salmon')
plt.xlabel('Jumlah Rating')
plt.ylabel('User-ID')
plt.title('Top 10 Pengguna Paling Jarang Memberikan Rating')
plt.gca().invert_yaxis()
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()

data['user_idx'] = data['User-ID'].astype('category').cat.codes.values
data['book_idx'] = data['ISBN'].astype('category').cat.codes.values

data['Book-Rating'] = data['Book-Rating'] / 10.0

X = data[['user_idx', 'book_idx']]
y = data['Book-Rating']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train_tensor = torch.tensor(X_train.values, dtype=torch.long)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test.values, dtype=torch.long)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)

def weighted_mse_loss(output, target):
    weight = torch.where(target > 0, 5.0, 1.0)
    loss = weight * (output - target) ** 2
    return loss.mean()

class CollaborativeFilteringModel(nn.Module):
    def __init__(self, num_users, num_books, embedding_dim):
        super(CollaborativeFilteringModel, self).__init__()
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.book_embedding = nn.Embedding(num_books, embedding_dim)
        self.fc1 = nn.Linear(embedding_dim * 2, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)
        self.relu = nn.ReLU()

    def forward(self, user_idx, book_idx):
        user_embedded = self.user_embedding(user_idx)
        book_embedded = self.book_embedding(book_idx)
        x = torch.cat([user_embedded, book_embedded], dim=-1)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

num_users = data['user_idx'].nunique()
num_books = data['book_idx'].nunique()
embedding_dim = 100
model = CollaborativeFilteringModel(num_users, num_books, embedding_dim)

criterion = weighted_mse_loss
optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)

epochs = 50
batch_size = 256
patience = 3
best_val_loss = float('inf')
patience_counter = 0

training_losses = []
validation_losses = []

print("Training started...")
for epoch in range(epochs):
    model.train()
    epoch_loss = 0
    for i in range(0, len(X_train_tensor), batch_size):
        batch_X = X_train_tensor[i:i+batch_size]
        batch_y = y_train_tensor[i:i+batch_size]
        user_idx = batch_X[:, 0]
        book_idx = batch_X[:, 1]
        outputs = model(user_idx, book_idx).squeeze()
        loss = criterion(outputs, batch_y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    avg_training_loss = epoch_loss / len(X_train_tensor)
    training_losses.append(avg_training_loss)

    model.eval()
    val_loss = 0
    with torch.no_grad():
        for i in range(0, len(X_test_tensor), batch_size):
            batch_X = X_test_tensor[i:i+batch_size]
            batch_y = y_test_tensor[i:i+batch_size]
            user_idx = batch_X[:, 0]
            book_idx = batch_X[:, 1]
            outputs = model(user_idx, book_idx).squeeze()
            loss = criterion(outputs, batch_y)
            val_loss += loss.item()
    avg_val_loss = val_loss / len(X_test_tensor)
    validation_losses.append(avg_val_loss)

    print(f"Epoch {epoch+1}/{epochs}, Training Loss: {avg_training_loss:.4f}, Validation Loss: {avg_val_loss:.4f}")

    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        patience_counter = 0
        torch.save(model.state_dict(), 'best_model.pth')
    else:
        patience_counter += 1
    if patience_counter >= patience:
        print("Early stopping triggered.")
        break

model.load_state_dict(torch.load('best_model.pth'))

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(training_losses) + 1), training_losses, label='Training Loss', marker='o')
plt.plot(range(1, len(validation_losses) + 1), validation_losses, label='Validation Loss', marker='o')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss')
plt.legend()
plt.grid()
plt.show()

def recommend_books_with_details(user_id, model, data, n=10):
    user_data = data[data['User-ID'] == user_id]
    top_rated_books = user_data.sort_values(by='Book-Rating', ascending=False).head(5)[['Book-Title', 'Book-Author', 'Publisher', 'Book-Rating']]
    user_idx = user_data['user_idx'].iloc[0]
    all_books = torch.tensor(data['book_idx'].unique(), dtype=torch.long)
    user_tensor = torch.tensor([user_idx] * len(all_books), dtype=torch.long)

    model.eval()
    with torch.no_grad():
        predictions = model(user_tensor, all_books).squeeze()

    top_books = torch.argsort(predictions, descending=True)[:n]
    book_indices = all_books[top_books].numpy()
    recommended_books = data[data['book_idx'].isin(book_indices)][['Book-Title', 'Book-Author', 'Publisher']].drop_duplicates()

    print(f"\n{'=' * 60}")
    print(f"{'Recommendations for User-ID: ' + str(user_id):^60}")
    print(f"{'=' * 60}\n")

    print(f"{'Top 5 Books Rated by User':^60}")
    print(f"{'-' * 60}")
    if top_rated_books.empty:
        print(f"{'No rated books found for this user.':^60}")
    else:
        for idx, row in top_rated_books.iterrows():
            print(f"- {row['Book-Title']} by {row['Book-Author']} ({row['Publisher']}) [Rating: {row['Book-Rating'] * 10:.1f}]")

    print(f"\n{'=' * 60}")
    print(f"{'Top ' + str(n) + ' Recommended Books':^60}")
    print(f"{'-' * 60}")
    for idx, row in recommended_books.head(n).iterrows():
        print(f"{idx+1:2}. {row['Book-Title']} by {row['Book-Author']} ({row['Publisher']})")
    print(f"{'=' * 60}\n")

user_id = 11676
recommend_books_with_details(user_id, model, data, n=10)

"""# **Model Content-Based Filtering**

**Penjelasan Kode**

Pada tahap ini, saya membangun model *content-based filtering* untuk memberikan rekomendasi buku berdasarkan kemiripan konten, yaitu penulis buku (*author*). Penjelasan langkah-langkah berikut:

1. **Load Dataset**
   - Dataset yang digunakan terdiri dari tiga file: `Users.csv`, `Ratings.csv`, dan `Books.csv`.
   - Dataset digabungkan untuk mengintegrasikan informasi buku, rating, dan pengguna.

2. **Menggabungkan Dataset**
   - Menggabungkan dataset berdasarkan kolom `ISBN` untuk mendapatkan informasi buku lengkap dan kolom `User-ID` untuk menyelaraskan data pengguna.

3. **Pembersihan Data**
   - Mengecek data null (*missing values*) dan menghapus baris yang mengandung nilai kosong.
   - Dataset akhir hanya menyisakan informasi relevan seperti `Book-Title`, `Book-Author`, dan kolom lainnya yang digunakan untuk model.

4. **TF-IDF Vectorization**
   - Menerapkan *TF-IDF (Term Frequency-Inverse Document Frequency)* pada kolom `Book-Author` untuk menghitung bobot kata yang unik dalam nama penulis.
   - Hasil vektorisasi adalah matriks yang mewakili setiap penulis dalam bentuk vektor.

5. **Cosine Similarity**
   - Menghitung *cosine similarity* untuk mengukur kemiripan antar buku berdasarkan vektor TF-IDF.
   - Hasilnya adalah matriks kemiripan berbentuk persegi, di mana setiap nilai menunjukkan skor kemiripan antar dua buku.

6. **Fungsi Rekomendasi**
   - Membuat fungsi `get_book_recommendations` untuk memberikan rekomendasi buku.
   - Fungsi ini:
     - Mengambil judul buku sebagai input.
     - Mencari buku-buku dengan skor kemiripan tertinggi berdasarkan matriks kemiripan (*cosine similarity*).
     - Mengembalikan daftar buku yang mirip dengan judul input.

7. **Contoh Penggunaan**
   - Judul buku seperti *"Harry Potter a l'ecole des sorciers"* digunakan sebagai input.
   - Fungsi menghasilkan daftar buku yang mirip berdasarkan penulis.

**Hasil Visualisasi**

1. **Rekomendasi Berdasarkan Judul Buku**
   - Buku yang direkomendasikan berdasarkan judul *"Along Came a Spider (Alex Cross Novels)"*:
     - Buku yang memiliki penulis yang sama, seperti *James Patterson*, ditampilkan.
   - Buku yang direkomendasikan berdasarkan *"Harry Potter and the Order of the Phoenix"*:
     - Hasil menunjukkan buku-buku lain dari penulis *J.K. Rowling*, termasuk versi bahasa berbeda.

**Kelebihan Content-Based Filtering**
- Rekomendasi akurat untuk buku yang memiliki karakteristik serupa (penulis atau atribut konten lainnya).
- Tidak bergantung pada data pengguna lain sehingga cocok untuk pengguna baru yang memiliki sedikit data rating (*cold start problem*).

**Catatan**
Pendekatan ini efektif jika metadata buku, seperti nama penulis atau deskripsi, tersedia secara lengkap dan akurat. Namun, tidak memperhitungkan preferensi pengguna secara langsung.
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

users_path = '/content/Users.csv'
ratings_path = '/content/Ratings.csv'
books_path = '/content/Books.csv'

users_df = pd.read_csv(users_path)
ratings_df = pd.read_csv(ratings_path)
books_df = pd.read_csv(books_path)

merged_books = books_df[['ISBN', 'Book-Title', 'Book-Author']].merge(ratings_df, on='ISBN')
merged_books = merged_books.merge(users_df, on='User-ID')

merged_books.head()

merged_books.info()

merged_books.isna().sum()

print("Jumlah duplikasi: ", merged_books.duplicated().sum())

merged_books = merged_books.dropna()

merged_books.isna().sum()

merged_books.info()

content_data = merged_books[['Book-Title', 'Book-Author']].drop_duplicates()

content_data.head()

tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(content_data['Book-Author'])

print(tfidf_matrix.shape)

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

cosine_sim_df = pd.DataFrame(cosine_sim, index=content_data['Book-Title'], columns=content_data['Book-Title'])

def get_book_recommendations(title, cosine_sim=cosine_sim_df, data=content_data, k=5):
    similar_books = cosine_sim[title].sort_values(ascending=False)[1:k+1]
    recommendations = data[data['Book-Title'].isin(similar_books.index)]
    return recommendations

title = "Harry Potter a l'ecole des sorciers"
recommendations = get_book_recommendations(title)
print(recommendations)